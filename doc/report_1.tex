\documentclass[a4paper, 10pt]{article}

\usepackage[a4paper, total={6in, 8.5in}]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{graphicx} 
\usepackage{verbatim} 
\usepackage{float}
\usepackage{xfrac}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{bm}

\usepackage{multicol}
\usepackage{sectsty,textcase}

\usepackage{wrapfig,lipsum,booktabs}
\bibliographystyle{plain}
% \renewcommand{\thesubsection}{\alph{subsection})}
\renewcommand{\thesection}{\arabic{section}}



%\setlength\parindent{0pt}	

\title{FYS-STK3155/4155 Applied Data Analysis and Machine Learning - Project 1: Regression analysis and resampling methods }

\author{Bernhard Nornes Lotsberg, Anh-Nguyet Lise Nguyen}
\date{September 2019}
\begin{document}
\maketitle


\begin{abstract} %\noindent
    this project blah blah blah\\
    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce et semper nibh, in vehicula sapien. In ultricies mollis enim, non venenatis odio ullamcorper id. Nunc consequat vulputate tempus.
\end{abstract}

\begin{multicols}{2}
\section{Introduction}



Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce et semper nibh, in vehicula sapien. In ultricies mollis enim, non venenatis     odio ullamcorper id. Nunc consequat vulputate tempus. Proin tempus sodales diam. Vestibulum pulvinar, dui et cursus dignissim, est elit placerat metus, sit amet volutpat lacus nisi sed neque. Fusce ullamcorper, sapien sit amet mollis semper, tellus risus luctus mauris, scelerisque blandit justo metus nec dui. Etiam consectetur ex ut elit condimentum, vel dictum mi volutpat. Cras lectus ante, venenatis sit amet nunc sed, auctor convallis lacus. Praesent ut nisi arcu. Morbi blandit ligula non dignissim dapibus. Fusce arcu tellus, sodales at sollicitudin sit amet, rhoncus luctus ex. Integer eget sem quis metus vestibulum accumsan quis vel magna. Integer venenatis pellentesque ex. Aliquam nec semper sapien. Donec tempor vehicula leo, at dictum dui venenatis non.


\section{Theory}
\subsection{Linear regression methods}
If given a data set $\mathcal{L}$ consisting of data $\bm{X}_\mathcal{L} = \{(y_j,\ \bm{x}_j),\ j=0,\ \dots,\ n-1\}$, assuming the true data is on the form
\[
\bm{y} = f(\bm{x})+ \bm{\varepsilon} ,
\]
where $\bm{\varepsilon} \sim N(0,\ \sigma ^2)$ and $f$ is some function of a design matrix $\bm{X}\in R^{n\times p}$, then it is possible by using linear regression to fit a linear model of $p$ degrees with outcome $\bm{\tilde{y}}$ on the form
\[
\bm{\tilde{y}} = \bm{X}\bm{\beta},
\]
with $\bm{\beta}$ being a $(p\times 1)$ vector containing the linear regression parameters.
It is possible to approach this problem using various different regression methods. One of the simplest methods is the Ordinary Least Squares (OLS) regression, with optimal  parameters $\hat{B}^\text{OLS}$ given by
\begin{equation}
    \hat{\beta}^\text{OLS} = \text{argmin}_{ {\beta} } \left[ \frac{1}{n} \sum_i^n (y_i - X_{i*}^T \beta)^2 \right],
    \label{eq:argminbeta_OLS}
\end{equation}
giving an analytical expression for the optimal regression parameters
\begin{equation}
    \hat{\beta}^{\text{OLS}} = (\bm{X}^T\bm{X})^{-1} \bm{X}^T \bm{y}.
    \label{eq:beta_OLS}
\end{equation}
The optimal regression parameters $\hat{\beta}^\text{ridge}$ for Ridge regression are given by
\begin{equation}
    \hat{\beta}^\text{ridge} = \text{argmin}_\beta \left[ \frac{1}{n}\sum_i^n(y_i-X_{i*}^T\beta)^2 + \lambda ||\beta||_2^2  \right]
    \label{eq:argminbeta_ridge}
\end{equation}
with the analytical optimal regression parameters
\begin{equation}
    \hat{\beta}^\text{ridge} = (\bm{X}^T\bm{X} +\lambda \bm{I})^{-1} \bm{X}^T \bm{y}.
    \label{eq:beta_ridge}
\end{equation}
The Least Absolute Shrinkage and Selection Operator (LASSO) regression minimizes
\begin{equation}
    \hat{\beta}^\text{lasso} =  \text{argmin}_\beta \left[  \frac{1}{n}\sum_i^n(y_i-X_{i*}^T\beta)^2 + \lambda ||\beta||_1  \right].
    \label{eq:argminbeta_lasso}
\end{equation}

\subsection{Bias-variance tradeoff}
In order to evaluate how our model fits the data, it can be useful to consider the mean squared error (MSE), defined as 
\begin{equation}
    MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
    \sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
    \label{eq:MSE}
\end{equation}
and the $R^2$ score, defined as 
\begin{equation}
    R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2}.
    \label{eq:R2}
\end{equation}
where $y$ is the data, $\tilde{y}$ is our fitted data and $\bar{y}$ is the mean of $y$.  The $R^2$ score will lie within the interval $[0,\ 1]$, and is a measure of the variance of our model and how well it fits the data, with 1 being the best possible score. 

In order to find the regression parameters $\bm{B}$ for the OLS regression method, you optimize the expression for the MSE. This is called the cost function, 
\begin{align*}
C(\bm{X},\bm{\beta} ) &= \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2 = \mathbb{E}[	(\bm{y}-\bm{\tilde{y}})^2] \\
\end{align*}
Using that the variance of $\bm{y}$ is $$\text{Var}[\bm{y}] = \mathbb{E}[\bm{y}^2]-\mathbb{E}[\bm{y}]^2=\sigma^2,$$ it can also be expressed as 
\begin{align*}
\mathbb{E}[	(\bm{y}-\bm{\tilde{y}})^2] =& \mathbb{E}[\bm{y}^2  -2\bm{y}\bm{\tilde{y}}+ \bm { \tilde{y} } ^2 ] \\
=& \mathbb{E}[\bm{y}^2]  - 2\mathbb{E}[\bm{y\tilde{y}}]+\mathbb{E}[\bm{\tilde{y}}^2]\\
=& \text{Var}[\bm{y}] + \mathbb{E}[\bm{y}]^2-2\mathbb{E}[\bm{y\tilde{y}}]+\mathbb{E}[\bm{\tilde{y}}^2]\\
=& \sigma^2 +  \mathbb{E}[\bm{y}]^2  -2 \mathbb{E}[\bm{y}\bm{\tilde{y}}] +  \mathbb{E}[\bm{\tilde{y}}^2]\\
=& \sigma^2 +  \mathbb{E}[f(\bm{x}) + \bm{\varepsilon}]^2 \\&- 2 \mathbb{E} [(f(\bm{x})+\varepsilon)\bm{\tilde{y}}] +  \mathbb{E}[\bm{\tilde{y}}^2 ]\\
=& \sigma^2 +  \mathbb{E}[f(\bm{x})]^2 -2f(\bm{x}) \mathbb{E}[\bm{\tilde{y}}] +  \mathbb{E}[\bm{\tilde{y}}] \\
=&\sigma^2 + f^2(\bm{x})-2f(\bm{x}) \mathbb{E}[\bm{\tilde{y}}] +  \mathbb{E}[\bm{\tilde{y}}]\\&+\mathbb{E}[\bm{\tilde{y}}]^2 + \mathbb{E}[\bm{\tilde{y}}]^2 -2\mathbb{E}[\bm{\tilde{y}}]^2\\
=&  \sigma ^2  + ( f(\bm{x}) -\mathbb{E}[\bm{\tilde{y}}]  )^2 + \mathbb{E}[\bm{\tilde{y}}^2] \\&+\mathbb{E}[\bm{\tilde{y}}]\mathbb{E}[\bm{\tilde{y}}]  - 2\mathbb{E}[\bm{\tilde{y}}]\mathbb{E}[\bm{\tilde{y}}]\\
= &  \sigma^2 +  ( f(\bm{x}) -\mathbb{E}[\bm{\tilde{y}}]  )^2   \\&+ \mathbb{E}[\bm{\tilde{y}}^2 + \mathbb{E}[\bm{\tilde{y}}]^2 - 2\bm{\tilde{y}}\mathbb{E}[\bm{\tilde{y}}]]\\
=&  \sigma^2 +  ( f(\bm{x}) -\mathbb{E}[\bm{\tilde{y}}]  )^2  +\mathbb{E}[(\bm{\tilde{y}} -\mathbb{E}[\bm{\tilde{y}}])^2] \\
\end{align*}
\begin{align}
 \mathbb{E}[	(\bm{y}-\bm{\tilde{y}})^2]  =&\sigma^2 + \frac{1}{n}\sum_i(f_i-\mathbb{E}\left[\bm{\tilde{y}}\right])^2  \nonumber \\
 &+ \frac{1}{n}\sum_i(\tilde{y}_i-\mathbb{E}\left[\bm{\tilde{y}}\right])^2.
\end{align}



\subsection{Resampling}
Finding a proper estimated fit for the given data can be challenging when the amount of available samples is limited. To evaluate the validity of the fitted model, resampling proves to be a powerful tool. Several methods for resampling exist, but the general idea of resampling is based on gaining additional information by repeatedly drawing arbitrary samples from the training set and refitting a model on each sample set. It is then possible to compare these new fits and examine how well they correspond to one another.

One method of resampling is called the $k$-fold cross-validation method, which gives us an estimate of the prediction error of our fitted model. $k$-fold cross-validation is based on first splitting the train data into $k$ approximately equal-sized sets.  We then run our regression method on $k-1$ of the $k$ folds, keeping one fold as the validation set, with the $k-1$ remaining folds as the training data. This is iterated $k$ times in a way that ensures that each of the $k$ folds is used as a validation set once. 



\section{Method}
To get an understanding of the three regression methods, we initially use Franke's function to study how to model a fit for two-dimensional data using all three methods. The function is given by 
\begin{align}
f(x,y) &= \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4}   - \frac{(9y-2)^2}{4}\right)} \nonumber\\
 &+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \nonumber\\
 &+\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} \nonumber\\
 &-\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
\end{align} which is defined on $x,\ y \in [0,1]$. 









\section{Results}
Donec elementum tempor eros, in gravida massa scelerisque ut. Vestibulum facilisis metus at ornare commodo. Donec lacus eros, dapibus id nunc id, suscipit ullamcorper justo. In vel felis mauris. Integer ultricies dolor quis vehicula ultricies. Pellentesque mattis elit quis tempus ullamcorper. Donec a nulla orci. Quisque pretium mauris vitae ipsum aliquet, quis malesuada est viverra. Maecenas quis consequat diam, vitae hendrerit tellus. Donec vulputate erat massa, id sollicitudin tellus cursus ut. Aliquam dui justo, interdum ut efficitur suscipit, ullamcorper eget nisi. Ut gravida, tortor at ornare suscipit, lacus tellus malesuada metus, vel lacinia justo lorem in nisl. Donec et sem eget lacus egestas auctor. Integer egestas, lorem eget volutpat malesuada, est nunc efficitur mi, ac pulvinar elit libero quis risus. Aenean convallis aliquam eros, interdum molestie sapien pretium sit amet. Aliquam at aliquam diam.


\end{multicols}

\begin{figure}[H]
    \includegraphics[scale=1]{figs/biasvariancetradeoff_ols_Franke.eps}
    \caption{Prediction error of training and test set plotted as a function of model complexity (polynomial degree) for the ordinary least squares regression.}
    \label{fig:my_label}
\end{figure}
    
\begin{figure}[H]
    \includegraphics{figs/biasvariancetradeoff_Ridge_Franke.eps}
    \caption{Prediction error of training and test set plotted as a function of the hyperparameter $\lambda$ for Ridge regression.}
\end{figure}    

\begin{figure}[H]
    \includegraphics{figs/biasvariancetradeoff_LASSO_Franke.eps}
    \caption{Prediction error of training and test set plotted as a function of the hyperparameter $\lambda$ for Lasso regression.}
\end{figure} 

\begin{multicols}{2}

Donec elementum tempor eros, in gravida massa scelerisque ut. Vestibulum facilisis metus at ornare commodo. Donec lacus eros, dapibus id nunc id, suscipit ullamcorper justo. In vel felis mauris. Integer ultricies dolor quis vehicula ultricies. Pellentesque mattis elit quis tempus ullamcorper. Donec a nulla orci. Quisque pretium mauris vitae ipsum aliquet, quis malesuada est viverra. Maecenas quis consequat diam, vitae hendrerit tellus. Donec vulputate erat massa, id sollicitudin tellus cursus ut. Aliquam dui justo, interdum ut efficitur suscipit, ullamcorper eget nisi. Ut gravida, tortor at ornare suscipit, lacus tellus malesuada metus, vel lacinia justo lorem in nisl. Donec et sem eget lacus egestas auctor. Integer egestas, lorem eget volutpat malesuada, est nunc efficitur mi, ac pulvinar elit libero quis risus. Aenean convallis aliquam eros, interdum molestie sapien pretium sit amet. Aliquam at aliquam diam.



\section{Discussion}
Ut egestas eget urna mollis consequat. Pellentesque blandit, leo auctor sodales porttitor, ipsum sapien commodo sapien, non tincidunt augue nunc eget nibh. Nulla ut arcu et ex condimentum placerat vel eget lacus. Quisque rhoncus eget elit vitae ornare. Suspendisse tempor, tortor vitae varius ullamcorper, eros diam tempor quam, vitae dignissim nulla nulla et lectus. Proin placerat vulputate vulputate. Morbi gravida, mauris eu auctor ultrices, turpis lacus elementum sapien, ut mollis nulla velit sed lacus. Aenean non nulla sit amet sapien tempus scelerisque sit amet ac lacus. Quisque eleifend consectetur ante, sit amet suscipit lacus pellentesque in. Suspendisse feugiat euismod risus, eu posuere libero dignissim id. Etiam quis molestie magna. Curabitur nibh nibh, tincidunt at dictum sed, aliquet a ligula. Vestibulum ut imperdiet nisi, a efficitur odio. Suspendisse interdum posuere tellus ut sagittis.

\section{Conclusion}
Duis malesuada sagittis mi ut gravida. Maecenas porta sed mi at malesuada. Nunc et rhoncus nisi, sit amet mattis mauris. Cras lacinia lobortis odio quis ullamcorper. Morbi volutpat turpis est, sit amet efficitur velit viverra ut. Vestibulum eu imperdiet mi. Mauris tincidunt tempor scelerisque. In scelerisque mauris justo, rutrum consectetur tortor cursus in. Vivamus odio diam, interdum eget lobortis ac, vestibulum eu diam. Integer lectus est, pharetra varius ligula et, sodales interdum arcu. Etiam volutpat dolor sit amet justo pharetra, fringilla dictum risus scelerisque.
\end{multicols}



\end{document}